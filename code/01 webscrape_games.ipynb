{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape NFL Games, Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Directory\n",
    "WORKING_DIRECTORY = \"/Users/peterchristenson/Desktop/Projects/NFL Weather Analysis/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set relevant web scraping info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasons to check\n",
    "seasons = list(range(2000, 2023))\n",
    "\n",
    "# Main webpage\n",
    "webpage_main = \"https://www.profootballarchives.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = pd.DataFrame()\n",
    "for season in seasons:\n",
    "    game_yr_df = pd.DataFrame()\n",
    "    try:\n",
    "        webpage = webpage_main + str(season) + \"nfl-boxscores.html\"\n",
    "        response = requests.get(webpage)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Get links to games\n",
    "        links_raw = soup.find_all(\"a\")\n",
    "        links = []\n",
    "        for link in links_raw:\n",
    "            if \"nflboxscores\" in link[\"href\"]:\n",
    "                links.append(link[\"href\"])\n",
    "            \n",
    "        print(links[0])        \n",
    "        # Loop through each game, pull attributes\n",
    "        for link in links:\n",
    "            try:\n",
    "                webpage = \"https://www.profootballarchives.com\" + link\n",
    "                response = requests.get(webpage)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                teams = soup.find(\"title\") # Teams playing game\n",
    "                date = None\n",
    "                location = None\n",
    "                venue = None\n",
    "                weather = None\n",
    "                for attr in soup.find_all(\"td\"):\n",
    "                    attr_text = attr.get_text()\n",
    "                    if \"Date\" in attr_text:\n",
    "                        date = attr_text\n",
    "                    if \"Location\" in attr_text:\n",
    "                        location = attr_text\n",
    "                    if \"Venue\" in attr_text:\n",
    "                        venue = attr_text\n",
    "                    if \"Weather:\" in attr_text:\n",
    "                        weather = attr_text\n",
    "                        \n",
    "                # Points\n",
    "                points = soup.find_all(\"table\")\n",
    "                score_table = [pt for pt in points if \"Score By Quarters\" in pt.get_text()]\n",
    "                score_table = score_table[0].find_all(\"td\", {\"style\":\"text-align:right\"})\n",
    "                team_1_pts = score_table[int(len(score_table)/2) - 1].get_text()\n",
    "                team_2_pts = score_table[len(score_table) - 1].get_text()\n",
    "                \n",
    "                # Add to main dataframe\n",
    "                df = pd.DataFrame(\n",
    "                    {'teams':[teams],\n",
    "                    'team 1 pts':[team_1_pts],\n",
    "                    'team 2 pts':[team_2_pts],\n",
    "                    'date':[date],\n",
    "                    'location':[location],\n",
    "                    'venue':[venue],\n",
    "                    'weather':[weather]\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                game_df = pd.concat([game_df, df])\n",
    "                game_yr_df = pd.concat([game_yr_df, df])\n",
    "                    \n",
    "                time.sleep(0.1) # Time buffer between webpage calls\n",
    "                print(link)\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        continue\n",
    "    game_yr_df.to_csv(WORKING_DIRECTORY + \"intermediate/\" + str(season) + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
